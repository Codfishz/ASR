{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2111,"status":"ok","timestamp":1744831318783,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"M4lIkMLt25Gd","outputId":"5c18debc-38e5-46d1-b6a1-a77fca52cedc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ASR'...\n","warning: redirecting to https://github.com/Codfishz/ASR.git/\n","remote: Enumerating objects: 58, done.\u001b[K\n","remote: Counting objects: 100% (58/58), done.\u001b[K\n","remote: Compressing objects: 100% (41/41), done.\u001b[K\n","remote: Total 58 (delta 25), reused 45 (delta 16), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (58/58), 10.84 MiB | 14.56 MiB/s, done.\n","Resolving deltas: 100% (25/25), done.\n"]}],"source":["# Example: My preferred approach\n","import os\n","# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n","os.environ['GITHUB_TOKEN'] = \"\"\n","\n","GITHUB_USERNAME = \"Codfishz\"\n","REPO_NAME       = \"ASR\"\n","TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n","repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n","!git clone {repo_url}"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1744831319193,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"8aq-iEXz4sFA","outputId":"1604a854-db94-45ed-c195-09ee7f3cacda"},"outputs":[{"output_type":"stream","name":"stdout","text":["warning: redirecting to https://github.com/Codfishz/ASR.git/\n","Already up to date.\n"]}],"source":["!cd {REPO_NAME} && git pull"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1744831319196,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"t-8t_7PV5SuT"},"outputs":[],"source":["import os\n","os.chdir('ASR/Script')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66701,"status":"ok","timestamp":1744831385898,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"dxy6Mzaf4ckC","outputId":"8bf679a4-ec74-4ef8-9963-06216381fd8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kaggle==1.7.4.2\n","  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n","Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n","\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/173.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.7.4.2\n","    Uninstalling kaggle-1.7.4.2:\n","      Successfully uninstalled kaggle-1.7.4.2\n","Successfully installed kaggle-1.7.4.2\n","Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/crop-pest-and-disease-detection\n","License(s): CC0-1.0\n","‚úÖ Total images found in dataset: 25220\n","Images before filter: 25220\n"]},{"output_type":"stream","name":"stderr","text":["Saving train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20176/20176 [00:34<00:00, 580.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","train split summary:\n","  Total images: 20103\n","\n"]},{"output_type":"stream","name":"stderr","text":["Saving val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2522/2522 [00:04<00:00, 586.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","val split summary:\n","  Total images: 2511\n","\n"]},{"output_type":"stream","name":"stderr","text":["Saving test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2522/2522 [00:04<00:00, 585.13it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","test split summary:\n","  Total images: 2512\n","\n","  Images after filter : 25126\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["%run \"Datapreparation_YOLO.ipynb\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80503,"status":"ok","timestamp":1744831466403,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"b_ZfVhgCZbg6","outputId":"7061b4d7-b27a-4903-f989-023b3e264415"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","Setup complete ‚úÖ (12 CPUs, 53.0 GB RAM, 43.7/235.7 GB disk)\n"]}],"source":["!pip install ultralytics\n","!pip install torchinfo\n","\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1744831466441,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"-Qg5l7w5vAI4","outputId":"08f9256f-abb5-460a-a071-dd707bff4a7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: True\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from ultralytics import YOLO\n","print(\"GPU available:\", torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"CwDCG0rPhogV"},"source":["# Active YOLO functions"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1744831466445,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"Iz11Fvjg2M3o"},"outputs":[],"source":["def setup_active_dataset():\n","\n","    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n","    active_train_dir = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR)\n","    os.makedirs(active_train_dir, exist_ok=True)\n","\n","    for cls in os.listdir(orig_train_dir):\n","        cls_path = os.path.join(orig_train_dir, cls)\n","        if os.path.isdir(cls_path):\n","            os.makedirs(os.path.join(active_train_dir, cls), exist_ok=True)\n","    print(f\"Complete creating train_dir {active_train_dir}\")\n","\n","    orig_val_dir = os.path.join(ORIGINAL_DATASET_DIR, VAL_SUBDIR)\n","    active_val_dir = os.path.join(ACTIVE_DATASET_DIR, VAL_SUBDIR)\n","    if os.path.exists(orig_val_dir):\n","        if os.path.exists(active_val_dir):\n","            shutil.rmtree(active_val_dir)\n","        shutil.copytree(orig_val_dir, active_val_dir)\n","\n","    orig_test_dir = os.path.join(ORIGINAL_DATASET_DIR, TEST_SUBDIR)\n","    active_test_dir = os.path.join(ACTIVE_DATASET_DIR, TEST_SUBDIR)\n","    if os.path.exists(orig_test_dir):\n","        if os.path.exists(active_test_dir):\n","            shutil.rmtree(active_test_dir)\n","        shutil.copytree(orig_test_dir, active_test_dir)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1744831466459,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"Eg5Ysn_b4Q9D"},"outputs":[],"source":["def get_all_samples():\n","\n","    samples = []\n","    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n","    for cls in os.listdir(orig_train_dir):\n","        cls_path = os.path.join(orig_train_dir, cls)\n","        if os.path.isdir(cls_path):\n","            image_files = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n","            for f in image_files:\n","                samples.append((cls, f))\n","    return samples"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1744831466462,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"TwCFkNEx4LqF"},"outputs":[],"source":["import shutil\n","def copy_samples(sample_list):\n","\n","    for cls, file_name in sample_list:\n","        src_path = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n","        dst_path = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n","        if not os.path.exists(dst_path):\n","            shutil.copy(src_path, dst_path)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1744831466464,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"CnqhHfvI5AXe"},"outputs":[],"source":["import random\n","def stratified_sample(all_samples, n_initial):\n","\n","    samples_by_class = {}\n","    for cls, filename in all_samples:\n","        samples_by_class.setdefault(cls, []).append((cls, filename))\n","\n","    stratified = []\n","    for cls, samples in samples_by_class.items():\n","        stratified.append(random.choice(samples))\n","    remaining_count = n_initial - len(stratified)\n","    if remaining_count > 0:\n","        remaining_samples = list(set(all_samples) - set(stratified))\n","        additional_samples = random.sample(remaining_samples, remaining_count)\n","        stratified.extend(additional_samples)\n","\n","    return stratified\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VAWYwDFPhogZ"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1744831466466,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"71DA5ol-iN2N"},"outputs":[],"source":["ORIGINAL_DATASET_DIR = '/content/data_yolo'\n","ACTIVE_DATASET_DIR = '/content/data_active'\n","\n","TRAIN_SUBDIR = 'train'\n","VAL_SUBDIR = 'val'\n","TEST_SUBDIR = 'test'\n","Num_Train = 20103\n","N_INITIAL = int(0.3 * Num_Train)\n","N_PER_PHASE = int(0.1 * Num_Train)\n","NUM_PHASES = 5"]},{"cell_type":"markdown","metadata":{"id":"ySABrtf0hoga"},"source":["# MC-Dropout"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"2ab0vIGQhoga","executionInfo":{"status":"ok","timestamp":1744831469508,"user_tz":240,"elapsed":3041,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from PIL import Image\n","from torchvision import transforms\n","import numpy as np\n","from tqdm import tqdm\n","\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    transforms.ToTensor(),\n","])\n","\n","def preprocess_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    tensor = transform(image)\n","    return tensor.cuda() if torch.cuda.is_available() else tensor\n","\n","\n","def mc_dropout_inference(model, images, num_passes=2, device='cuda'):\n","    \"\"\"\n","    Perform MC Dropout inference and return mean + variance across predictions.\n","    \"\"\"\n","    model.train()\n","    for module in model.modules():\n","        if isinstance(module, nn.BatchNorm2d):\n","            module.eval()\n","\n","    images = images.to(device)\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for _ in range(num_passes):\n","            pred = model(images)  # [B, num_classes]\n","            predictions.append(pred)\n","\n","    preds_tensor = torch.stack(predictions)  # [T, B, num_classes]\n","    mean_preds = preds_tensor.mean(dim=0)    # [B, num_classes]\n","    var_preds = preds_tensor.var(dim=0).mean(dim=1)  # [B] average variance per sample\n","\n","    return mean_preds, var_preds"]},{"cell_type":"code","source":["def select_uncertain_samples(unlabeled_images, model, num_passes=2, k=10, batch_size=32, original_dir=\"/content/data_yolo/train\", device='cuda'):\n","    \"\"\"\n","    Batched version of MC Dropout uncertainty sampling.\n","    Args:\n","        unlabeled_images: list of (class_name, filename)\n","        model: PyTorch model\n","        Returns:\n","            selected_images, uncertainty_scores, selected_indices\n","    \"\"\"\n","    model.to(device)\n","\n","    paths = [os.path.join(original_dir, cls, fn) for cls, fn in unlabeled_images]\n","    uncertainty_scores = []\n","    all_indices = []\n","\n","    for i in tqdm(range(0, len(paths), batch_size), desc=\"Evaluating batches\"):\n","        batch_paths = paths[i:i+batch_size]\n","        batch_samples = unlabeled_images[i:i+batch_size]\n","\n","        batch_tensors = []\n","        valid_indices = []\n","\n","        for j, path in enumerate(batch_paths):\n","            if not os.path.exists(path):\n","                print(f\"[Warning] Missing file: {path}\")\n","                continue\n","            try:\n","                tensor = preprocess_image(path)\n","                batch_tensors.append(tensor)\n","                valid_indices.append(i + j)\n","            except Exception as e:\n","                print(f\"[Error] Failed to load {path}: {e}\")\n","\n","        if not batch_tensors:\n","            continue\n","\n","        batch_tensor = torch.stack(batch_tensors)  # [B, C, H, W]\n","        _, batch_uncertainty = mc_dropout_inference(model, batch_tensor, num_passes=num_passes, device=device)\n","\n","        uncertainty_scores.extend(batch_uncertainty.cpu().tolist())\n","        all_indices.extend(valid_indices)\n","\n","    # Final selection\n","    uncertainty_scores = np.array(uncertainty_scores)\n","    all_indices = np.array(all_indices)\n","    top_indices_within_valid = np.argsort(uncertainty_scores)[-k:]\n","    selected_indices = all_indices[top_indices_within_valid]\n","    selected_images = [unlabeled_images[i] for i in selected_indices]\n","\n","    return selected_images, uncertainty_scores, selected_indices.tolist()"],"metadata":{"id":"4uM3tvSc_8jz","executionInfo":{"status":"ok","timestamp":1744831469513,"user_tz":240,"elapsed":2,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"o3PH8saZhoga","executionInfo":{"status":"ok","timestamp":1744831469515,"user_tz":240,"elapsed":1,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["# from tqdm import tqdm\n","\n","# def select_uncertain_samples(unlabeled_images, model, num_passes=5, k=10, original_dir=\"/content/data_yolo/train\"):\n","#     \"\"\"\n","#     Select k most uncertain images from unlabeled_images using MC Dropout.\n","#     \"\"\"\n","#     uncertainty_scores = []\n","\n","#     for class_name, filename in tqdm(unlabeled_images, desc=\"Evaluating uncertainty\"):\n","#         image_path = os.path.join(original_dir, class_name, filename)\n","#         image_tensor = preprocess_image(image_path)\n","#         _, uncertainty = mc_dropout_inference(model, image_tensor, num_passes=num_passes)\n","#         score = uncertainty.mean().item()\n","#         uncertainty_scores.append(score)\n","\n","#     uncertainty_scores = np.array(uncertainty_scores)\n","#     selected_indices = np.argsort(uncertainty_scores)[-k:]  # top-k uncertain\n","#     selected_images = [unlabeled_images[i] for i in selected_indices]\n","\n","#     return selected_images, uncertainty_scores, selected_indices\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1045415,"status":"ok","timestamp":1744832514932,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"},"user_tz":240},"id":"_SU_i9Lcc0XU","outputId":"f91b818e-808d-472a-c934-861c001f2c15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Complete creating train_dir /content/data_active/train\n","Initial Epoch\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.52M/5.52M [00:00<00:00, 98.3MB/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase0, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 6030 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n","Overriding model.yaml nc=80 with nc=22\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 234/236 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/active-phase0', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.35M/5.35M [00:00<00:00, 95.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 6030 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6030/6030 [00:01<00:00, 3369.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2511 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [00:00<00:00, 2763.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/val/Tomato leaf curl/leaf curl185_.jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data_active/val.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase0\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      2.02G      3.286         16        640:   2%|‚ñè         | 8/377 [00:01<00:43,  8.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      2.02G      3.243         16        640:   4%|‚ñç         | 16/377 [00:02<00:35, 10.30it/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 19.7MB/s]\n","        1/1      2.02G      2.017         14        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 377/377 [00:37<00:00, 10.03it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:08<00:00,  9.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.713      0.975\n","\n","1 epochs completed in 0.015 hours.\n","Optimizer stripped from runs/classify/active-phase0/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase0/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase0/weights/best.pt...\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 6030 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.713      0.975\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase0\u001b[0m\n","Phase 1: selecting top 2010 uncertain samples...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440/440 [03:10<00:00,  2.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Phase 2: add 2010 samples, total sample number: 8040\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase0/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase1\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 8040 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/active-phase1', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 8040 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8040/8040 [00:02<00:00, 3337.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2511 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/val/Tomato leaf curl/leaf curl185_.jpg: corrupt JPEG restored and saved\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase1\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.41G     0.8917          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 503/503 [00:46<00:00, 10.86it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.763      0.993\n","\n","1 epochs completed in 0.017 hours.\n","Optimizer stripped from runs/classify/active-phase1/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase1/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase1/weights/best.pt...\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 8040 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.763      0.993\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase1\u001b[0m\n","Phase 2 training completed\n","Phase 2: selecting top 2010 uncertain samples...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 377/377 [02:43<00:00,  2.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Phase 3: add 2010 samples, total sample number: 10050\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase1/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase2\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 10050 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/active-phase2', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 10050 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10050/10050 [00:02<00:00, 3435.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/train/Maize leaf beetle/leaf beetle325_.jpg: corrupt JPEG restored and saved\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2511 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/val/Tomato leaf curl/leaf curl185_.jpg: corrupt JPEG restored and saved\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase2\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.55G     0.6914          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:57<00:00, 10.86it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:08<00:00,  9.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.815      0.996\n","\n","1 epochs completed in 0.020 hours.\n","Optimizer stripped from runs/classify/active-phase2/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase2/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase2/weights/best.pt...\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 10050 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.815      0.996\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase2\u001b[0m\n","Phase 3 training completed\n","Phase 3: selecting top 2010 uncertain samples...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 315/315 [02:07<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Phase 4: add 2010 samples, total sample number: 12060\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase2/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase3\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 12060 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/active-phase3', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 12060 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12060/12060 [00:03<00:00, 3447.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2511 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/val/Tomato leaf curl/leaf curl185_.jpg: corrupt JPEG restored and saved\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase3\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1       1.4G     0.5916         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 754/754 [01:07<00:00, 11.12it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:08<00:00,  9.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.826      0.996\n","\n","1 epochs completed in 0.023 hours.\n","Optimizer stripped from runs/classify/active-phase3/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase3/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase3/weights/best.pt...\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 12060 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00,  9.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.825      0.995\n","Speed: 0.5ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase3\u001b[0m\n","Phase 4 training completed\n","Phase 4: selecting top 2010 uncertain samples...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 252/252 [01:41<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Phase 5: add 2010 samples, total sample number: 14070\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase3/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase4\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 14070 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/active-phase4', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 14070 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14070/14070 [00:04<00:00, 3445.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2511 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/data_active/val/Tomato leaf curl/leaf curl185_.jpg: corrupt JPEG restored and saved\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase4\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.44G     0.5369          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [01:20<00:00, 10.88it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.848      0.997\n","\n","1 epochs completed in 0.026 hours.\n","Optimizer stripped from runs/classify/active-phase4/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase4/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase4/weights/best.pt...\n","Ultralytics 8.3.109 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 14070 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2511 images in 22 classes ‚úÖ \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2512 images in 22 classes ‚úÖ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:07<00:00, 10.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.849      0.997\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase4\u001b[0m\n","Phase 5 training completed\n"]}],"source":["import csv\n","\n","# Initialize dataset\n","setup_active_dataset()\n","all_samples = get_all_samples()\n","\n","# Prepare CSV logging\n","log_path = \"accuracy_log.csv\"\n","with open(log_path, \"w\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Phase\", \"Num_Images\", \"Top-1 Accuracy\", \"Top-5 Accuracy\"])\n","\n","# Initial sampling and train\n","current_sample_list = stratified_sample(all_samples, N_INITIAL)\n","copy_samples(current_sample_list)\n","\n","print(\"Initial Epoch\")\n","model = YOLO('yolo11n-cls.pt')\n","results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=\"active-phase0\", project=\"runs/classify\")\n","\n","# Log initial accuracy\n","acc1 = results.top1\n","acc5 = results.top5\n","\n","with open(log_path, \"a\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([0, len(current_sample_list), acc1, acc5])\n","\n","# Compute remaining samples\n","remaining_samples = list(set(all_samples) - set(current_sample_list))\n","\n","# Active Learning (Uncertainty Sampling)\n","for phase in range(1, NUM_PHASES):\n","    n_to_add = min(N_PER_PHASE, len(remaining_samples))\n","    if n_to_add <= 0:\n","        print(\"No more samples\")\n","        break\n","\n","    print(f\"Phase {phase}: selecting top {n_to_add} uncertain samples...\")\n","\n","    # Load model from previous phase\n","    model = YOLO(f\"runs/classify/active-phase{phase-1}/weights/last.pt\")\n","\n","    # Apply uncertainty sampling using MC Dropout\n","    # Send to GPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.model = model.model.to(device)\n","    model.model.train()  # Enable dropout\n","    for module in model.model.modules():\n","        if isinstance(module, nn.BatchNorm2d):\n","            module.eval()\n","\n","    selected_images, uncertainty_scores, selected_indices = select_uncertain_samples(\n","        remaining_samples,\n","        model.model,\n","        num_passes=2,\n","        k=n_to_add,\n","        batch_size=32,\n","        device=device\n","    )\n","\n","    new_samples = [remaining_samples[i] for i in selected_indices]\n","    copy_samples(new_samples)\n","\n","    current_sample_list.extend(new_samples)\n","    remaining_samples = list(set(remaining_samples) - set(new_samples))\n","\n","    print(f\"Phase {phase}: add {n_to_add} samples, total sample number: {len(current_sample_list)}\")\n","\n","    # Train model for this phase\n","    results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=f\"active-phase{phase}\", project=\"runs/classify\")\n","\n","    # Log accuracy\n","    acc1 = results.top1\n","    acc5 = results.top5\n","\n","    with open(log_path, \"a\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([phase, len(current_sample_list), acc1, acc5])\n","\n","    print(f\"Phase {phase+1} training completed\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"X8-f1BbPhoga","executionInfo":{"status":"ok","timestamp":1744832514936,"user_tz":240,"elapsed":2,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Lzi7wscCemHS","executionInfo":{"status":"ok","timestamp":1744832514959,"user_tz":240,"elapsed":22,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"collapsed_sections":["CwDCG0rPhogV"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}