{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5odPXUk4iso"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "34iJrhoU2IuR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4dkyvngVk6T",
        "outputId": "ad7ead44-8f62-47ad-fb8d-afbbc482c632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ASR'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 89 (delta 43), reused 62 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (89/89), 12.26 MiB | 17.27 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"\"\n",
        "\n",
        "GITHUB_USERNAME = \"Codfishz\"\n",
        "REPO_NAME       = \"ASR\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyCemzBtVk6T",
        "outputId": "cb74775d-0811-4bc9-d0ff-d1bed95a0673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OvFAb4GKVk6U"
      },
      "outputs": [],
      "source": [
        "os.chdir('ASR/Script')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8yE4LJAVk6U",
        "outputId": "486bd27c-e234-4d8a-bf0b-de3d9913c1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.7.4.2\n",
            "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/173.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.7.4.2\n",
            "    Uninstalling kaggle-1.7.4.2:\n",
            "      Successfully uninstalled kaggle-1.7.4.2\n",
            "Successfully installed kaggle-1.7.4.2\n",
            "Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/crop-pest-and-disease-detection\n",
            "License(s): CC0-1.0\n",
            "âœ… Total images found in dataset: 25220\n",
            "Images before filter: 25220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20176/20176 [00:31<00:00, 635.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "train split summary:\n",
            "  Total images: 20093\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2522/2522 [00:03<00:00, 647.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "val split summary:\n",
            "  Total images: 2514\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2522/2522 [00:03<00:00, 649.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "test split summary:\n",
            "  Total images: 2519\n",
            "\n",
            "  Images after filter : 25126\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%run \"Datapreparation_YOLO.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-a7Vh0gocUZ",
        "outputId": "b70b9ac4-9fd2-49e7-fed1-3df7dc507263"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing 'Cashew anthracnose': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1729/1729 [00:02<00:00, 715.41it/s]\n",
            "Processing 'Cashew gumosis': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392/392 [00:00<00:00, 597.76it/s]\n",
            "Processing 'Cashew healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1368/1368 [00:01<00:00, 768.86it/s]\n",
            "Processing 'Cashew leaf miner': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1378/1378 [00:01<00:00, 765.29it/s]\n",
            "Processing 'Cashew red rust': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1682/1682 [00:02<00:00, 644.38it/s]\n",
            "Processing 'Cassava bacterial blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2614/2614 [00:03<00:00, 794.73it/s]\n",
            "Processing 'Cassava brown spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1481/1481 [00:01<00:00, 777.97it/s]\n",
            "Processing 'Cassava green mite': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1015/1015 [00:01<00:00, 766.53it/s]\n",
            "Processing 'Cassava healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1193/1193 [00:01<00:00, 796.44it/s]\n",
            "Processing 'Cassava mosaic': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1205/1205 [00:01<00:00, 812.77it/s]\n",
            "Processing 'Maize fall armyworm': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 822.94it/s]\n",
            "Processing 'Maize grasshoper': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 881.65it/s]\n",
            "Processing 'Maize healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:00<00:00, 836.11it/s]\n",
            "Processing 'Maize leaf beetle': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 948/948 [00:00<00:00, 966.22it/s]\n",
            "Processing 'Maize leaf blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1006/1006 [00:01<00:00, 756.85it/s]\n",
            "Processing 'Maize leaf spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1259/1259 [00:01<00:00, 767.95it/s]\n",
            "Processing 'Maize streak virus': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 979/979 [00:01<00:00, 737.54it/s]\n",
            "Processing 'Tomato healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 470/470 [00:00<00:00, 802.12it/s]\n",
            "Processing 'Tomato leaf blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1301/1301 [00:01<00:00, 780.30it/s]\n",
            "Processing 'Tomato leaf curl': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 805.17it/s]\n",
            "Processing 'Tomato septoria leaf spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2743/2743 [00:03<00:00, 798.10it/s]\n",
            "Processing 'Tomato verticulium wilt': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 773/773 [00:00<00:00, 786.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Categories\n",
        "categories = sorted([d for d in os.listdir(original_base) if os.path.isdir(os.path.join(original_base, d))])\n",
        "\n",
        "# Set image path and labels\n",
        "image_paths = []\n",
        "image_labels = []\n",
        "\n",
        "base_path = \"/content/data\"\n",
        "\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(base_path, category)\n",
        "    filenames = [f for f in os.listdir(category_dir) if f.endswith(\".jpg\")]\n",
        "    for filename in tqdm(filenames, desc=f\"Processing '{category}'\"):\n",
        "      image_path = os.path.join(category_dir, filename)\n",
        "      try:\n",
        "          img = Image.open(image_path)\n",
        "          img = img.convert(\"RGB\")\n",
        "          img.load()  # load image each time to remove the damaged image thoroughly\n",
        "          image_paths.append(image_path)\n",
        "          image_labels.append(category)\n",
        "      except (UnidentifiedImageError, OSError):\n",
        "          continue\n",
        "\n",
        "# Label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(image_labels)\n",
        "\n",
        "# Dataset\n",
        "class CropDiseaseDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # To fit the pretrain model (è¯´æ˜¯resnetçš„å®˜æ–¹æ¨èè¾“å…¥å¤§å°)\n",
        "    # transforms.RandomRotation(20),\n",
        "    # transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # To fit the pretrain model (æ²¡æœ‰å¾ˆæ‡‚å•Šï¼ŒåŠ ä¸Šä¹‹åå›¾ç‰‡ç¾¤é­”ä¹±èˆ)\n",
        "])\n",
        "\n",
        "\n",
        "# Make Dataset and DataLoader\n",
        "dataset = CropDiseaseDataset(image_paths, encoded_labels, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Visualization\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in dataloader:\n",
        "#     for i in range(9):\n",
        "#         img = images[i].permute(1, 2, 0).numpy()\n",
        "#         plt.subplot(3, 3, i+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.axis('off')\n",
        "#     break\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eATpJjs4nQ_Y",
        "outputId": "f1d28d16-e570-4933-b0a4-1ea59a118064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25126\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG8dsAAI4bQ7"
      },
      "source": [
        "# Feature extraction (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4TBZQvu10LH",
        "outputId": "cf591d44-a75f-4a74-b123-2d44998cb459"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 234MB/s]\n",
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [01:37<00:00,  1.01it/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained ResNet\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "resnet18 = resnet18(weights=weights)\n",
        "\n",
        "# Remove the final fully connected layer\n",
        "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Extract features\n",
        "\n",
        "def extract_features(dataloader, model, device):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            images = images.to(device)\n",
        "            features = model(images)                  # shape: [B, 512, 1, 1]\n",
        "            features = features.view(features.size(0), -1)  # flatten to [B, 512]\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features), torch.cat(all_labels)\n",
        "\n",
        "features, labels = extract_features(dataloader, feature_extractor, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Lolcnj4vos",
        "outputId": "9394aa48-3143-4344-b088-1d4e647bc3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: torch.Size([25126, 512])\n",
            "Labels shape: torch.Size([25126])\n"
          ]
        }
      ],
      "source": [
        "# Results\n",
        "print(\"Features shape:\", features.shape)  # [N, 512]\n",
        "print(\"Labels shape:\", labels.shape)      # [N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l_C56__M4v36"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, to_tree\n",
        "\n",
        "# features: numpy array, shape (N, D)\n",
        "Z = linkage(features, method='ward')\n",
        "root, nodes_list = to_tree(Z, rd=True)\n",
        "\n",
        "pruning = [root]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p0p7wmcTnTVD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def select_dh_cluster(pruning, labeled_indices, labels, alpha=0.05):\n",
        "    \"\"\"\n",
        "    pruning: å½“å‰çš„èŠ‚ç‚¹åˆ—è¡¨\n",
        "    labeled_indices: dictï¼Œä»æ ·æœ¬ idx åˆ°æ ‡ç­¾\n",
        "    labels: å®Œæ•´æ ‡ç­¾æ•°ç»„ï¼Œç”¨äºè·å– ground truth\n",
        "    \"\"\"\n",
        "    best_score, best_node = -1, None\n",
        "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
        "    for node in pruning:\n",
        "        leaves = node.get_leaves()\n",
        "        # æ‰¾å‡ºè¿™ä¸ªèŠ‚ç‚¹ä¸‹å·²ç»æ ‡æ³¨çš„æ ·æœ¬\n",
        "        L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "        if len(L)==0:\n",
        "            score = len(leaves)  # æ— ä¿¡æ¯æ—¶ä¼˜å…ˆå¤§èŠ‚ç‚¹\n",
        "        else:\n",
        "            # å¤šæ•°æ ‡ç­¾é¢‘ç‡\n",
        "            p_hat = max(L.count(c)/len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            # ç½®ä¿¡ä¸‹ç•Œ\n",
        "            lower = p_hat - z*(p_hat*(1-p_hat)/n)**0.5\n",
        "            eps = 1 - max(lower, 0)\n",
        "            score = eps * len(leaves)\n",
        "        if score > best_score:\n",
        "            best_score, best_node = score, node\n",
        "    # ä» best_node çš„ leaves ä¸­éšæœºæŠ½ä¸€ä¸ªæœªæ ‡è®°æ ·æœ¬\n",
        "    candidates = [i for i in best_node.get_leaves() if i not in labeled_indices]\n",
        "    sample_idx = random.choice(candidates)\n",
        "    return sample_idx, best_node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CWB_g9pRnTVD"
      },
      "outputs": [],
      "source": [
        "def update_pruning(pruning, sampled_node, labeled_indices, alpha=0.05):\n",
        "    new_pruning = []\n",
        "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
        "    for node in pruning:\n",
        "        if node is sampled_node:\n",
        "            # è®¡ç®—è¯¥èŠ‚ç‚¹ä¸‹æ ‡æ³¨åå¤šæ•°æ ‡ç­¾é¢‘ç‡å’Œç½®ä¿¡åŒºé—´\n",
        "            leaves = node.get_leaves()\n",
        "            L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "            p_hat = max(L.count(c)/len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            lower = p_hat - z*(p_hat*(1-p_hat)/n)**0.5\n",
        "            eps = 1 - lower\n",
        "            # å¦‚æœçº¯åº¦ä¸å¤Ÿï¼ˆeps è¾ƒå¤§ï¼‰ï¼Œåˆ™æ‹†åˆ†\n",
        "            if eps > some_threshold:   # e.g., eps > 0.1\n",
        "                new_pruning.extend([node.get_left(), node.get_right()])\n",
        "                continue\n",
        "        new_pruning.append(node)\n",
        "    return new_pruning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xhara8jBrEA5"
      },
      "outputs": [],
      "source": [
        "def get_leaves(node):\n",
        "    # Leaf node: left/right éƒ½æ˜¯ None\n",
        "    if node.left is None and node.right is None:\n",
        "        return [node.id]\n",
        "    leaves = []\n",
        "    if node.left:\n",
        "        leaves += get_leaves(node.left)\n",
        "    if node.right:\n",
        "        leaves += get_leaves(node.right)\n",
        "    return leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t84cNrOVk6X",
        "outputId": "99138b4e-866f-4ea8-9132-6b2c230040cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Setup complete âœ… (12 CPUs, 53.0 GB RAM, 43.7/235.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install torchinfo\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtqqZhMVk6X"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LSbDNPPaVk6X"
      },
      "outputs": [],
      "source": [
        "def setup_active_dataset():\n",
        "\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    active_train_dir = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    os.makedirs(active_train_dir, exist_ok=True)\n",
        "\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            os.makedirs(os.path.join(active_train_dir, cls), exist_ok=True)\n",
        "    print(f\"Complete creating train_dir {active_train_dir}\")\n",
        "\n",
        "    orig_val_dir = os.path.join(ORIGINAL_DATASET_DIR, VAL_SUBDIR)\n",
        "    active_val_dir = os.path.join(ACTIVE_DATASET_DIR, VAL_SUBDIR)\n",
        "    if os.path.exists(orig_val_dir):\n",
        "        if os.path.exists(active_val_dir):\n",
        "            shutil.rmtree(active_val_dir)\n",
        "        shutil.copytree(orig_val_dir, active_val_dir)\n",
        "\n",
        "    orig_test_dir = os.path.join(ORIGINAL_DATASET_DIR, TEST_SUBDIR)\n",
        "    active_test_dir = os.path.join(ACTIVE_DATASET_DIR, TEST_SUBDIR)\n",
        "    if os.path.exists(orig_test_dir):\n",
        "        if os.path.exists(active_test_dir):\n",
        "            shutil.rmtree(active_test_dir)\n",
        "        shutil.copytree(orig_test_dir, active_test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LWsTUCDFVk6Y"
      },
      "outputs": [],
      "source": [
        "def get_all_samples():\n",
        "\n",
        "    samples = []\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            image_files = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n",
        "            for f in image_files:\n",
        "                samples.append((cls, f))\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xEMoCCC7Vk6Y"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "def copy_samples(sample_list):\n",
        "\n",
        "    for cls, file_name in sample_list:\n",
        "        src_path = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        dst_path = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copy(src_path, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z3JIgEElVk6Y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def stratified_sample(all_samples, n_initial):\n",
        "\n",
        "    samples_by_class = {}\n",
        "    for cls, filename in all_samples:\n",
        "        samples_by_class.setdefault(cls, []).append((cls, filename))\n",
        "\n",
        "    stratified = []\n",
        "    for cls, samples in samples_by_class.items():\n",
        "        stratified.append(random.choice(samples))\n",
        "    remaining_count = n_initial - len(stratified)\n",
        "    if remaining_count > 0:\n",
        "        remaining_samples = list(set(all_samples) - set(stratified))\n",
        "        additional_samples = random.sample(remaining_samples, remaining_count)\n",
        "        stratified.extend(additional_samples)\n",
        "\n",
        "    return stratified\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6GPmEfVk6Y"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T-9Q6koEVk6Y"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "ORIGINAL_DATASET_DIR = '/content/data_yolo'\n",
        "ACTIVE_DATASET_DIR = '/content/data_active'\n",
        "\n",
        "TRAIN_SUBDIR = 'train'\n",
        "VAL_SUBDIR = 'val'\n",
        "TEST_SUBDIR = 'test'\n",
        "Num_Train = 20093\n",
        "N_INITIAL = int(0.3 * Num_Train)\n",
        "N_PER_PHASE = int(0.1 * Num_Train)\n",
        "NUM_PHASES = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuv8_j6DVk6Z"
      },
      "source": [
        "# DH selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mNGjvhsiVk6Z",
        "outputId": "b7082b11-f025-4983-9f95-2b9f020302ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete creating train_dir /content/data_active/train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:20<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Epoch\n",
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase04, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase04\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15923 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n",
            "Overriding model.yaml nc=80 with nc=22\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2093.3Â±769.0 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15923 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15923/15923 [00:04<00:00, 3475.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 575.3Â±93.1 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase04\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      3.33G      1.551          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 996/996 [01:34<00:00, 10.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.801      0.996\n",
            "\n",
            "1 epochs completed in 0.030 hours.\n",
            "Optimizer stripped from runs/classify/active-phase04/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/classify/active-phase04/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/classify/active-phase04/weights/best.pt...\n",
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15923 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.801      0.996\n",
            "Speed: 0.5ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/active-phase04\u001b[0m\n",
            "[DH Phase 1] added 1 sample, total=6028\n",
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase0/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase1\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 236/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2027.0Â±696.5 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15924 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15924/15924 [00:04<00:00, 3375.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 593.7Â±92.3 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase1\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      1.81G      0.755          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 996/996 [01:34<00:00, 10.56it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.814      0.997\n",
            "\n",
            "1 epochs completed in 0.029 hours.\n",
            "Optimizer stripped from runs/classify/active-phase1/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/classify/active-phase1/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/classify/active-phase1/weights/best.pt...\n",
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 11.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.814      0.997\n",
            "Speed: 0.5ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/active-phase1\u001b[0m\n",
            "Phase 1 training completed\n",
            "[DH Phase 2] added 1 sample, total=6029\n",
            "Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase1/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase2\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 236/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2054.8Â±659.5 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15924 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15924/15924 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 623.2Â±392.3 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase2\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      1.79G     0.6678         16        640:   3%|â–         | 28/996 [00:04<02:19,  6.95it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2c162f71abe3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# (d) ç”¨ YOLO åœ¨ç´¯ç§¯æ ·æœ¬ä¸Šå¾®è°ƒå¹¶è®°å½•å‡†ç¡®ç‡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"runs/classify/active-phase{phase-1}/weights/last.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACTIVE_DATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mni\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/torch_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# true for FP16 and FP32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmsd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0;31m# assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype},  model {msd[k].dtype}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "from scipy.cluster.hierarchy import linkage, to_tree\n",
        "import scipy.stats as stats\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Helper to collect leaf indices from a ClusterNode ---\n",
        "def get_leaves(node):\n",
        "    if node.left is None and node.right is None:\n",
        "        return [node.id]\n",
        "    leaves = []\n",
        "    if node.left is not None:\n",
        "        leaves += get_leaves(node.left)\n",
        "    if node.right is not None:\n",
        "        leaves += get_leaves(node.right)\n",
        "    return leaves\n",
        "\n",
        "# --- 1) æ„å»º/åˆå§‹åŒ– Active æ•°æ®é›† & æå– all_samples åˆ—è¡¨ ---\n",
        "setup_active_dataset()\n",
        "all_samples = get_all_samples()\n",
        "\n",
        "# --- 2) åˆå§‹åˆ†å±‚éšæœºé‡‡æ · & æ‹·è´åˆ° active_data ---\n",
        "current_sample_list = stratified_sample(all_samples, N_INITIAL)\n",
        "copy_samples(current_sample_list)\n",
        "\n",
        "# --- 3) å…¨æ± ç‰¹å¾æå– & æ„å»ºå±‚æ¬¡èšç±»æ ‘ ---\n",
        "# æ„é€ å®Œæ•´ Dataset & DataLoader\n",
        "full_paths  = [os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, fn)\n",
        "               for cls, fn in all_samples]\n",
        "full_labels = [label for label in encoded_labels]\n",
        "full_dataset = CropDiseaseDataset(full_paths, full_labels, transform=transform)\n",
        "full_loader  = DataLoader(full_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "# æå–ç‰¹å¾\n",
        "full_features, full_labels = extract_features(full_loader, feature_extractor, device)\n",
        "features_np  = full_features.numpy()  # shape: (N, D)\n",
        "labels_np    = full_labels.numpy()    # shape: (N,)\n",
        "\n",
        "# ç”¨ Ward æ–¹æ³•åšå±‚æ¬¡èšç±»\n",
        "Z = linkage(features_np, method='ward')\n",
        "root, _ = to_tree(Z, rd=True)\n",
        "\n",
        "# åˆå§‹åŒ– DH çš„ pruning åˆ—è¡¨å’Œæ ‡æ³¨å­—å…¸\n",
        "pruning = [root]\n",
        "labeled_indices = {}\n",
        "for cls, fn in current_sample_list:\n",
        "    idx = all_samples.index((cls, fn))\n",
        "    labeled_indices[idx] = labels_np[idx]\n",
        "\n",
        "# --- 4) é¦–è½® YOLO è®­ç»ƒ & æ—¥å¿—åˆå§‹åŒ– ---\n",
        "log_path = \"accuracy_log_dh.csv\"\n",
        "with open(log_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Phase\", \"Num_Images\", \"Top-1\", \"Top-5\"])\n",
        "\n",
        "print(\"Initial Epoch\")\n",
        "model = YOLO('yolo11n-cls.pt')\n",
        "results = model.train(\n",
        "    data=ACTIVE_DATASET_DIR,\n",
        "    epochs=1, imgsz=640,\n",
        "    name=\"active-phase0\", project=\"runs/classify\"\n",
        ")\n",
        "with open(log_path, \"a\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([0, len(current_sample_list), results.top1, results.top5])\n",
        "\n",
        "# æ„é€  remaining_samples æ± \n",
        "remaining_samples = list(set(all_samples) - set(current_sample_list))\n",
        "\n",
        "# --- 5) DH ä¸»å¾ªç¯ ---\n",
        "alpha = 0.05\n",
        "z = stats.norm.ppf(1 - alpha / 2)\n",
        "for phase in range(1, NUM_PHASES):\n",
        "    if not remaining_samples:\n",
        "        print(\"No more samples\")\n",
        "        break\n",
        "\n",
        "    # (a) åœ¨ pruning ä¸­é€‰å‡ºå¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹\n",
        "    best_score, best_node = -1, None\n",
        "    for node in pruning:\n",
        "        leaves = get_leaves(node)\n",
        "        L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "        if not L:\n",
        "            score = len(leaves)\n",
        "        else:\n",
        "            p_hat = max(L.count(c) / len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            lower = p_hat - z * (p_hat * (1 - p_hat) / n) ** 0.5\n",
        "            eps = 1 - max(lower, 0)\n",
        "            score = eps * len(leaves)\n",
        "        if score > best_score:\n",
        "            best_score, best_node = score, node\n",
        "\n",
        "    # (b) åœ¨ best_node ä¸‹éšæœºé€‰ä¸€ä¸ªæœªæ ‡æ³¨æ ·æœ¬\n",
        "    candidates = [i for i in get_leaves(best_node) if i not in labeled_indices]\n",
        "    sample_idx = random.choice(candidates)\n",
        "    cls, fn = all_samples[sample_idx]\n",
        "    copy_samples([(cls, fn)])                  # æ‹·è´å•å¼ å›¾ç‰‡\n",
        "    labeled_indices[sample_idx] = labels_np[sample_idx]\n",
        "    current_sample_list.append((cls, fn))\n",
        "    remaining_samples.remove((cls, fn))\n",
        "\n",
        "    # (c) æ›´æ–° pruningï¼šè‹¥çº¯åº¦ä¸å¤Ÿåˆ™æ‹†åˆ†è¯¥èŠ‚ç‚¹\n",
        "    leaves = get_leaves(best_node)\n",
        "    L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "    if L:\n",
        "        p_hat = max(L.count(c) / len(L) for c in set(L))\n",
        "        lower = p_hat - z * (p_hat * (1 - p_hat) / len(L)) ** 0.5\n",
        "        eps = 1 - max(lower, 0)\n",
        "    else:\n",
        "        eps = 1.0\n",
        "    if eps > 0.1 and best_node.left and best_node.right:\n",
        "        pruning.remove(best_node)\n",
        "        pruning.extend([best_node.left, best_node.right])\n",
        "\n",
        "    print(f\"[DH Phase {phase}] added 1 sample, total={len(current_sample_list)}\")\n",
        "\n",
        "    # (d) ç”¨ YOLO åœ¨ç´¯ç§¯æ ·æœ¬ä¸Šå¾®è°ƒå¹¶è®°å½•å‡†ç¡®ç‡\n",
        "    model = YOLO(f\"runs/classify/active-phase{phase-1}/weights/last.pt\")\n",
        "    results = model.train(\n",
        "        data=ACTIVE_DATASET_DIR,\n",
        "        epochs=1, imgsz=640,\n",
        "        name=f\"active-phase{phase}\", project=\"runs/classify\"\n",
        "    )\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([phase, len(current_sample_list), results.top1, results.top5])\n",
        "\n",
        "    print(f\"Phase {phase} training completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDF6WZQtVk6Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IBtqqZhMVk6X"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4280832,
          "sourceId": 7368427,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
