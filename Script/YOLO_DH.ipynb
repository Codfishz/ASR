{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5odPXUk4iso"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "34iJrhoU2IuR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4dkyvngVk6T",
        "outputId": "ad7ead44-8f62-47ad-fb8d-afbbc482c632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ASR'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 89 (delta 43), reused 62 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (89/89), 12.26 MiB | 17.27 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"\"\n",
        "\n",
        "GITHUB_USERNAME = \"Codfishz\"\n",
        "REPO_NAME       = \"ASR\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyCemzBtVk6T",
        "outputId": "cb74775d-0811-4bc9-d0ff-d1bed95a0673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OvFAb4GKVk6U"
      },
      "outputs": [],
      "source": [
        "os.chdir('ASR/Script')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8yE4LJAVk6U",
        "outputId": "486bd27c-e234-4d8a-bf0b-de3d9913c1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.7.4.2\n",
            "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/173.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.7.4.2\n",
            "    Uninstalling kaggle-1.7.4.2:\n",
            "      Successfully uninstalled kaggle-1.7.4.2\n",
            "Successfully installed kaggle-1.7.4.2\n",
            "Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/crop-pest-and-disease-detection\n",
            "License(s): CC0-1.0\n",
            "✅ Total images found in dataset: 25220\n",
            "Images before filter: 25220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving train: 100%|██████████| 20176/20176 [00:31<00:00, 635.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "train split summary:\n",
            "  Total images: 20093\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving val: 100%|██████████| 2522/2522 [00:03<00:00, 647.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "val split summary:\n",
            "  Total images: 2514\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving test: 100%|██████████| 2522/2522 [00:03<00:00, 649.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "test split summary:\n",
            "  Total images: 2519\n",
            "\n",
            "  Images after filter : 25126\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%run \"Datapreparation_YOLO.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-a7Vh0gocUZ",
        "outputId": "b70b9ac4-9fd2-49e7-fed1-3df7dc507263"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing 'Cashew anthracnose': 100%|██████████| 1729/1729 [00:02<00:00, 715.41it/s]\n",
            "Processing 'Cashew gumosis': 100%|██████████| 392/392 [00:00<00:00, 597.76it/s]\n",
            "Processing 'Cashew healthy': 100%|██████████| 1368/1368 [00:01<00:00, 768.86it/s]\n",
            "Processing 'Cashew leaf miner': 100%|██████████| 1378/1378 [00:01<00:00, 765.29it/s]\n",
            "Processing 'Cashew red rust': 100%|██████████| 1682/1682 [00:02<00:00, 644.38it/s]\n",
            "Processing 'Cassava bacterial blight': 100%|██████████| 2614/2614 [00:03<00:00, 794.73it/s]\n",
            "Processing 'Cassava brown spot': 100%|██████████| 1481/1481 [00:01<00:00, 777.97it/s]\n",
            "Processing 'Cassava green mite': 100%|██████████| 1015/1015 [00:01<00:00, 766.53it/s]\n",
            "Processing 'Cassava healthy': 100%|██████████| 1193/1193 [00:01<00:00, 796.44it/s]\n",
            "Processing 'Cassava mosaic': 100%|██████████| 1205/1205 [00:01<00:00, 812.77it/s]\n",
            "Processing 'Maize fall armyworm': 100%|██████████| 285/285 [00:00<00:00, 822.94it/s]\n",
            "Processing 'Maize grasshoper': 100%|██████████| 673/673 [00:00<00:00, 881.65it/s]\n",
            "Processing 'Maize healthy': 100%|██████████| 208/208 [00:00<00:00, 836.11it/s]\n",
            "Processing 'Maize leaf beetle': 100%|██████████| 948/948 [00:00<00:00, 966.22it/s]\n",
            "Processing 'Maize leaf blight': 100%|██████████| 1006/1006 [00:01<00:00, 756.85it/s]\n",
            "Processing 'Maize leaf spot': 100%|██████████| 1259/1259 [00:01<00:00, 767.95it/s]\n",
            "Processing 'Maize streak virus': 100%|██████████| 979/979 [00:01<00:00, 737.54it/s]\n",
            "Processing 'Tomato healthy': 100%|██████████| 470/470 [00:00<00:00, 802.12it/s]\n",
            "Processing 'Tomato leaf blight': 100%|██████████| 1301/1301 [00:01<00:00, 780.30it/s]\n",
            "Processing 'Tomato leaf curl': 100%|██████████| 518/518 [00:00<00:00, 805.17it/s]\n",
            "Processing 'Tomato septoria leaf spot': 100%|██████████| 2743/2743 [00:03<00:00, 798.10it/s]\n",
            "Processing 'Tomato verticulium wilt': 100%|██████████| 773/773 [00:00<00:00, 786.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Categories\n",
        "categories = sorted([d for d in os.listdir(original_base) if os.path.isdir(os.path.join(original_base, d))])\n",
        "\n",
        "# Set image path and labels\n",
        "image_paths = []\n",
        "image_labels = []\n",
        "\n",
        "base_path = \"/content/data\"\n",
        "\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(base_path, category)\n",
        "    filenames = [f for f in os.listdir(category_dir) if f.endswith(\".jpg\")]\n",
        "    for filename in tqdm(filenames, desc=f\"Processing '{category}'\"):\n",
        "      image_path = os.path.join(category_dir, filename)\n",
        "      try:\n",
        "          img = Image.open(image_path)\n",
        "          img = img.convert(\"RGB\")\n",
        "          img.load()  # load image each time to remove the damaged image thoroughly\n",
        "          image_paths.append(image_path)\n",
        "          image_labels.append(category)\n",
        "      except (UnidentifiedImageError, OSError):\n",
        "          continue\n",
        "\n",
        "# Label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(image_labels)\n",
        "\n",
        "# Dataset\n",
        "class CropDiseaseDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # To fit the pretrain model (说是resnet的官方推荐输入大小)\n",
        "    # transforms.RandomRotation(20),\n",
        "    # transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # To fit the pretrain model (没有很懂啊，加上之后图片群魔乱舞)\n",
        "])\n",
        "\n",
        "\n",
        "# Make Dataset and DataLoader\n",
        "dataset = CropDiseaseDataset(image_paths, encoded_labels, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Visualization\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in dataloader:\n",
        "#     for i in range(9):\n",
        "#         img = images[i].permute(1, 2, 0).numpy()\n",
        "#         plt.subplot(3, 3, i+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.axis('off')\n",
        "#     break\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eATpJjs4nQ_Y",
        "outputId": "f1d28d16-e570-4933-b0a4-1ea59a118064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25126\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG8dsAAI4bQ7"
      },
      "source": [
        "# Feature extraction (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4TBZQvu10LH",
        "outputId": "cf591d44-a75f-4a74-b123-2d44998cb459"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 234MB/s]\n",
            "Extracting features: 100%|██████████| 99/99 [01:37<00:00,  1.01it/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained ResNet\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "resnet18 = resnet18(weights=weights)\n",
        "\n",
        "# Remove the final fully connected layer\n",
        "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Extract features\n",
        "\n",
        "def extract_features(dataloader, model, device):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            images = images.to(device)\n",
        "            features = model(images)                  # shape: [B, 512, 1, 1]\n",
        "            features = features.view(features.size(0), -1)  # flatten to [B, 512]\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features), torch.cat(all_labels)\n",
        "\n",
        "features, labels = extract_features(dataloader, feature_extractor, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Lolcnj4vos",
        "outputId": "9394aa48-3143-4344-b088-1d4e647bc3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: torch.Size([25126, 512])\n",
            "Labels shape: torch.Size([25126])\n"
          ]
        }
      ],
      "source": [
        "# Results\n",
        "print(\"Features shape:\", features.shape)  # [N, 512]\n",
        "print(\"Labels shape:\", labels.shape)      # [N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l_C56__M4v36"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, to_tree\n",
        "\n",
        "# features: numpy array, shape (N, D)\n",
        "Z = linkage(features, method='ward')\n",
        "root, nodes_list = to_tree(Z, rd=True)\n",
        "\n",
        "pruning = [root]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p0p7wmcTnTVD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def select_dh_cluster(pruning, labeled_indices, labels, alpha=0.05):\n",
        "    \"\"\"\n",
        "    pruning: 当前的节点列表\n",
        "    labeled_indices: dict，从样本 idx 到标签\n",
        "    labels: 完整标签数组，用于获取 ground truth\n",
        "    \"\"\"\n",
        "    best_score, best_node = -1, None\n",
        "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
        "    for node in pruning:\n",
        "        leaves = node.get_leaves()\n",
        "        # 找出这个节点下已经标注的样本\n",
        "        L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "        if len(L)==0:\n",
        "            score = len(leaves)  # 无信息时优先大节点\n",
        "        else:\n",
        "            # 多数标签频率\n",
        "            p_hat = max(L.count(c)/len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            # 置信下界\n",
        "            lower = p_hat - z*(p_hat*(1-p_hat)/n)**0.5\n",
        "            eps = 1 - max(lower, 0)\n",
        "            score = eps * len(leaves)\n",
        "        if score > best_score:\n",
        "            best_score, best_node = score, node\n",
        "    # 从 best_node 的 leaves 中随机抽一个未标记样本\n",
        "    candidates = [i for i in best_node.get_leaves() if i not in labeled_indices]\n",
        "    sample_idx = random.choice(candidates)\n",
        "    return sample_idx, best_node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CWB_g9pRnTVD"
      },
      "outputs": [],
      "source": [
        "def update_pruning(pruning, sampled_node, labeled_indices, alpha=0.05):\n",
        "    new_pruning = []\n",
        "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
        "    for node in pruning:\n",
        "        if node is sampled_node:\n",
        "            # 计算该节点下标注后多数标签频率和置信区间\n",
        "            leaves = node.get_leaves()\n",
        "            L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "            p_hat = max(L.count(c)/len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            lower = p_hat - z*(p_hat*(1-p_hat)/n)**0.5\n",
        "            eps = 1 - lower\n",
        "            # 如果纯度不够（eps 较大），则拆分\n",
        "            if eps > some_threshold:   # e.g., eps > 0.1\n",
        "                new_pruning.extend([node.get_left(), node.get_right()])\n",
        "                continue\n",
        "        new_pruning.append(node)\n",
        "    return new_pruning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xhara8jBrEA5"
      },
      "outputs": [],
      "source": [
        "def get_leaves(node):\n",
        "    # Leaf node: left/right 都是 None\n",
        "    if node.left is None and node.right is None:\n",
        "        return [node.id]\n",
        "    leaves = []\n",
        "    if node.left:\n",
        "        leaves += get_leaves(node.left)\n",
        "    if node.right:\n",
        "        leaves += get_leaves(node.right)\n",
        "    return leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t84cNrOVk6X",
        "outputId": "99138b4e-866f-4ea8-9132-6b2c230040cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Setup complete ✅ (12 CPUs, 53.0 GB RAM, 43.7/235.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install torchinfo\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtqqZhMVk6X"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LSbDNPPaVk6X"
      },
      "outputs": [],
      "source": [
        "def setup_active_dataset():\n",
        "\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    active_train_dir = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    os.makedirs(active_train_dir, exist_ok=True)\n",
        "\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            os.makedirs(os.path.join(active_train_dir, cls), exist_ok=True)\n",
        "    print(f\"Complete creating train_dir {active_train_dir}\")\n",
        "\n",
        "    orig_val_dir = os.path.join(ORIGINAL_DATASET_DIR, VAL_SUBDIR)\n",
        "    active_val_dir = os.path.join(ACTIVE_DATASET_DIR, VAL_SUBDIR)\n",
        "    if os.path.exists(orig_val_dir):\n",
        "        if os.path.exists(active_val_dir):\n",
        "            shutil.rmtree(active_val_dir)\n",
        "        shutil.copytree(orig_val_dir, active_val_dir)\n",
        "\n",
        "    orig_test_dir = os.path.join(ORIGINAL_DATASET_DIR, TEST_SUBDIR)\n",
        "    active_test_dir = os.path.join(ACTIVE_DATASET_DIR, TEST_SUBDIR)\n",
        "    if os.path.exists(orig_test_dir):\n",
        "        if os.path.exists(active_test_dir):\n",
        "            shutil.rmtree(active_test_dir)\n",
        "        shutil.copytree(orig_test_dir, active_test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LWsTUCDFVk6Y"
      },
      "outputs": [],
      "source": [
        "def get_all_samples():\n",
        "\n",
        "    samples = []\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            image_files = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n",
        "            for f in image_files:\n",
        "                samples.append((cls, f))\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xEMoCCC7Vk6Y"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "def copy_samples(sample_list):\n",
        "\n",
        "    for cls, file_name in sample_list:\n",
        "        src_path = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        dst_path = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copy(src_path, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z3JIgEElVk6Y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def stratified_sample(all_samples, n_initial):\n",
        "\n",
        "    samples_by_class = {}\n",
        "    for cls, filename in all_samples:\n",
        "        samples_by_class.setdefault(cls, []).append((cls, filename))\n",
        "\n",
        "    stratified = []\n",
        "    for cls, samples in samples_by_class.items():\n",
        "        stratified.append(random.choice(samples))\n",
        "    remaining_count = n_initial - len(stratified)\n",
        "    if remaining_count > 0:\n",
        "        remaining_samples = list(set(all_samples) - set(stratified))\n",
        "        additional_samples = random.sample(remaining_samples, remaining_count)\n",
        "        stratified.extend(additional_samples)\n",
        "\n",
        "    return stratified\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6GPmEfVk6Y"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T-9Q6koEVk6Y"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "ORIGINAL_DATASET_DIR = '/content/data_yolo'\n",
        "ACTIVE_DATASET_DIR = '/content/data_active'\n",
        "\n",
        "TRAIN_SUBDIR = 'train'\n",
        "VAL_SUBDIR = 'val'\n",
        "TEST_SUBDIR = 'test'\n",
        "Num_Train = 20093\n",
        "N_INITIAL = int(0.3 * Num_Train)\n",
        "N_PER_PHASE = int(0.1 * Num_Train)\n",
        "NUM_PHASES = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuv8_j6DVk6Z"
      },
      "source": [
        "# DH selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mNGjvhsiVk6Z",
        "outputId": "b7082b11-f025-4983-9f95-2b9f020302ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete creating train_dir /content/data_active/train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 79/79 [00:20<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Epoch\n",
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase04, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase04\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15923 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes ✅ \n",
            "Overriding model.yaml nc=80 with nc=22\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2093.3±769.0 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15923 images, 0 corrupt: 100%|██████████| 15923/15923 [00:04<00:00, 3475.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 575.3±93.1 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|██████████| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase04\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      3.33G      1.551          3        640: 100%|██████████| 996/996 [01:34<00:00, 10.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 79/79 [00:08<00:00,  9.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.801      0.996\n",
            "\n",
            "1 epochs completed in 0.030 hours.\n",
            "Optimizer stripped from runs/classify/active-phase04/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/classify/active-phase04/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/classify/active-phase04/weights/best.pt...\n",
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15923 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes ✅ \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|██████████| 79/79 [00:07<00:00, 10.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.801      0.996\n",
            "Speed: 0.5ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/active-phase04\u001b[0m\n",
            "[DH Phase 1] added 1 sample, total=6028\n",
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase0/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase1\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes ✅ \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 236/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2027.0±696.5 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15924 images, 0 corrupt: 100%|██████████| 15924/15924 [00:04<00:00, 3375.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 593.7±92.3 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|██████████| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase1\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      1.81G      0.755          4        640: 100%|██████████| 996/996 [01:34<00:00, 10.56it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 79/79 [00:07<00:00, 10.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.814      0.997\n",
            "\n",
            "1 epochs completed in 0.029 hours.\n",
            "Optimizer stripped from runs/classify/active-phase1/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/classify/active-phase1/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/classify/active-phase1/weights/best.pt...\n",
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes ✅ \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|██████████| 79/79 [00:07<00:00, 11.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.814      0.997\n",
            "Speed: 0.5ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/active-phase1\u001b[0m\n",
            "Phase 1 training completed\n",
            "[DH Phase 2] added 1 sample, total=6029\n",
            "Ultralytics 8.3.111 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase1/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase2\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 15924 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes ✅ \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n",
            "Transferred 236/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2054.8±659.5 MB/s, size: 64.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 15924 images, 0 corrupt: 100%|██████████| 15924/15924 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 623.2±392.3 MB/s, size: 66.7 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|██████████| 2514/2514 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/active-phase2\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      1.79G     0.6678         16        640:   3%|▎         | 28/996 [00:04<02:19,  6.95it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2c162f71abe3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# (d) 用 YOLO 在累积样本上微调并记录准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"runs/classify/active-phase{phase-1}/weights/last.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACTIVE_DATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mni\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/torch_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# true for FP16 and FP32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmsd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0;31m# assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype},  model {msd[k].dtype}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "from scipy.cluster.hierarchy import linkage, to_tree\n",
        "import scipy.stats as stats\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Helper to collect leaf indices from a ClusterNode ---\n",
        "def get_leaves(node):\n",
        "    if node.left is None and node.right is None:\n",
        "        return [node.id]\n",
        "    leaves = []\n",
        "    if node.left is not None:\n",
        "        leaves += get_leaves(node.left)\n",
        "    if node.right is not None:\n",
        "        leaves += get_leaves(node.right)\n",
        "    return leaves\n",
        "\n",
        "# --- 1) 构建/初始化 Active 数据集 & 提取 all_samples 列表 ---\n",
        "setup_active_dataset()\n",
        "all_samples = get_all_samples()\n",
        "\n",
        "# --- 2) 初始分层随机采样 & 拷贝到 active_data ---\n",
        "current_sample_list = stratified_sample(all_samples, N_INITIAL)\n",
        "copy_samples(current_sample_list)\n",
        "\n",
        "# --- 3) 全池特征提取 & 构建层次聚类树 ---\n",
        "# 构造完整 Dataset & DataLoader\n",
        "full_paths  = [os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, fn)\n",
        "               for cls, fn in all_samples]\n",
        "full_labels = [label for label in encoded_labels]\n",
        "full_dataset = CropDiseaseDataset(full_paths, full_labels, transform=transform)\n",
        "full_loader  = DataLoader(full_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "# 提取特征\n",
        "full_features, full_labels = extract_features(full_loader, feature_extractor, device)\n",
        "features_np  = full_features.numpy()  # shape: (N, D)\n",
        "labels_np    = full_labels.numpy()    # shape: (N,)\n",
        "\n",
        "# 用 Ward 方法做层次聚类\n",
        "Z = linkage(features_np, method='ward')\n",
        "root, _ = to_tree(Z, rd=True)\n",
        "\n",
        "# 初始化 DH 的 pruning 列表和标注字典\n",
        "pruning = [root]\n",
        "labeled_indices = {}\n",
        "for cls, fn in current_sample_list:\n",
        "    idx = all_samples.index((cls, fn))\n",
        "    labeled_indices[idx] = labels_np[idx]\n",
        "\n",
        "# --- 4) 首轮 YOLO 训练 & 日志初始化 ---\n",
        "log_path = \"accuracy_log_dh.csv\"\n",
        "with open(log_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Phase\", \"Num_Images\", \"Top-1\", \"Top-5\"])\n",
        "\n",
        "print(\"Initial Epoch\")\n",
        "model = YOLO('yolo11n-cls.pt')\n",
        "results = model.train(\n",
        "    data=ACTIVE_DATASET_DIR,\n",
        "    epochs=1, imgsz=640,\n",
        "    name=\"active-phase0\", project=\"runs/classify\"\n",
        ")\n",
        "with open(log_path, \"a\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([0, len(current_sample_list), results.top1, results.top5])\n",
        "\n",
        "# 构造 remaining_samples 池\n",
        "remaining_samples = list(set(all_samples) - set(current_sample_list))\n",
        "\n",
        "# --- 5) DH 主循环 ---\n",
        "alpha = 0.05\n",
        "z = stats.norm.ppf(1 - alpha / 2)\n",
        "for phase in range(1, NUM_PHASES):\n",
        "    if not remaining_samples:\n",
        "        print(\"No more samples\")\n",
        "        break\n",
        "\n",
        "    # (a) 在 pruning 中选出得分最高的节点\n",
        "    best_score, best_node = -1, None\n",
        "    for node in pruning:\n",
        "        leaves = get_leaves(node)\n",
        "        L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "        if not L:\n",
        "            score = len(leaves)\n",
        "        else:\n",
        "            p_hat = max(L.count(c) / len(L) for c in set(L))\n",
        "            n = len(L)\n",
        "            lower = p_hat - z * (p_hat * (1 - p_hat) / n) ** 0.5\n",
        "            eps = 1 - max(lower, 0)\n",
        "            score = eps * len(leaves)\n",
        "        if score > best_score:\n",
        "            best_score, best_node = score, node\n",
        "\n",
        "    # (b) 在 best_node 下随机选一个未标注样本\n",
        "    candidates = [i for i in get_leaves(best_node) if i not in labeled_indices]\n",
        "    sample_idx = random.choice(candidates)\n",
        "    cls, fn = all_samples[sample_idx]\n",
        "    copy_samples([(cls, fn)])                  # 拷贝单张图片\n",
        "    labeled_indices[sample_idx] = labels_np[sample_idx]\n",
        "    current_sample_list.append((cls, fn))\n",
        "    remaining_samples.remove((cls, fn))\n",
        "\n",
        "    # (c) 更新 pruning：若纯度不够则拆分该节点\n",
        "    leaves = get_leaves(best_node)\n",
        "    L = [labeled_indices[i] for i in leaves if i in labeled_indices]\n",
        "    if L:\n",
        "        p_hat = max(L.count(c) / len(L) for c in set(L))\n",
        "        lower = p_hat - z * (p_hat * (1 - p_hat) / len(L)) ** 0.5\n",
        "        eps = 1 - max(lower, 0)\n",
        "    else:\n",
        "        eps = 1.0\n",
        "    if eps > 0.1 and best_node.left and best_node.right:\n",
        "        pruning.remove(best_node)\n",
        "        pruning.extend([best_node.left, best_node.right])\n",
        "\n",
        "    print(f\"[DH Phase {phase}] added 1 sample, total={len(current_sample_list)}\")\n",
        "\n",
        "    # (d) 用 YOLO 在累积样本上微调并记录准确率\n",
        "    model = YOLO(f\"runs/classify/active-phase{phase-1}/weights/last.pt\")\n",
        "    results = model.train(\n",
        "        data=ACTIVE_DATASET_DIR,\n",
        "        epochs=1, imgsz=640,\n",
        "        name=f\"active-phase{phase}\", project=\"runs/classify\"\n",
        "    )\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([phase, len(current_sample_list), results.top1, results.top5])\n",
        "\n",
        "    print(f\"Phase {phase} training completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDF6WZQtVk6Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IBtqqZhMVk6X"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4280832,
          "sourceId": 7368427,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
