{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5odPXUk4iso"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34iJrhoU2IuR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"\"\n",
        "\n",
        "GITHUB_USERNAME = \"Codfishz\"\n",
        "REPO_NAME       = \"ASR\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir('ASR/Script')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"Datapreparation_YOLO.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G-a7Vh0gocUZ",
        "outputId": "7610d51b-38a6-49cf-a22b-0997b0ce174e"
      },
      "outputs": [],
      "source": [
        "# Categories\n",
        "categories = sorted([d for d in os.listdir(original_base) if os.path.isdir(os.path.join(original_base, d))])\n",
        "\n",
        "# Set image path and labels\n",
        "image_paths = []\n",
        "image_labels = []\n",
        "\n",
        "base_path = \"/content/data\"\n",
        "\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(base_path, category)\n",
        "    filenames = [f for f in os.listdir(category_dir) if f.endswith(\".jpg\")]\n",
        "    for filename in tqdm(filenames, desc=f\"Processing '{category}'\"):\n",
        "      image_path = os.path.join(category_dir, filename)\n",
        "      try:\n",
        "          img = Image.open(image_path)\n",
        "          img = img.convert(\"RGB\")\n",
        "          img.load()  # load image each time to remove the damaged image thoroughly\n",
        "          image_paths.append(image_path)\n",
        "          image_labels.append(category)\n",
        "      except (UnidentifiedImageError, OSError):\n",
        "          continue\n",
        "\n",
        "# Label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(image_labels)\n",
        "\n",
        "# Dataset\n",
        "class CropDiseaseDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # To fit the pretrain model (说是resnet的官方推荐输入大小)\n",
        "    # transforms.RandomRotation(20),\n",
        "    # transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # To fit the pretrain model (没有很懂啊，加上之后图片群魔乱舞)\n",
        "])\n",
        "\n",
        "\n",
        "# Make Dataset and DataLoader\n",
        "dataset = CropDiseaseDataset(image_paths, encoded_labels, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Visualization\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in dataloader:\n",
        "#     for i in range(9):\n",
        "#         img = images[i].permute(1, 2, 0).numpy()\n",
        "#         plt.subplot(3, 3, i+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.axis('off')\n",
        "#     break\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eATpJjs4nQ_Y",
        "outputId": "76fbca26-2732-43e2-e277-88a0cc1e14cb"
      },
      "outputs": [],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG8dsAAI4bQ7"
      },
      "source": [
        "# Feature extraction (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4TBZQvu10LH",
        "outputId": "a26c9f23-90e9-47b4-db90-92e0e3ae7062"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained ResNet\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "resnet18 = resnet18(weights=weights)\n",
        "\n",
        "# Remove the final fully connected layer\n",
        "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Extract features\n",
        "\n",
        "def extract_features(dataloader, model, device):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            images = images.to(device)\n",
        "            features = model(images)                  # shape: [B, 512, 1, 1]\n",
        "            features = features.view(features.size(0), -1)  # flatten to [B, 512]\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features), torch.cat(all_labels)\n",
        "\n",
        "features, labels = extract_features(dataloader, feature_extractor, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Lolcnj4vos",
        "outputId": "cf4eaf63-661e-40d8-8077-adac569138c7"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "print(\"Features shape:\", features.shape)  # [N, 512]\n",
        "print(\"Labels shape:\", labels.shape)      # [N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_C56__M4v36"
      },
      "outputs": [],
      "source": [
        "def select_density_samples_from_features(features, k=10):\n",
        "    \"\"\"\n",
        "    Select the top-k samples based on density computed from pre-extracted features.\n",
        "\n",
        "    Parameters:\n",
        "        features: A NumPy array of shape (N, D), where N is the number of images\n",
        "                  (e.g. 24453) and D is the feature dimension (e.g. 512).\n",
        "        k: The number of samples to select.\n",
        "\n",
        "    Returns:\n",
        "        selected_indices: The indices of the selected samples (top-k with highest density).\n",
        "        density_scores: A NumPy array of density scores for all samples.\n",
        "    \"\"\"\n",
        "    # Normalize the feature vectors to unit length (to use cosine similarity)\n",
        "    norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8  # avoid division by zero\n",
        "    norm_features = features / norms\n",
        "\n",
        "    # Compute the pairwise cosine similarity matrix.\n",
        "    # Note: Similarity between feature i and j is the dot product of normalized features.\n",
        "    similarity_matrix = np.dot(norm_features, norm_features.T)\n",
        "\n",
        "    # Remove self-similarity by setting the diagonal elements to 0.\n",
        "    np.fill_diagonal(similarity_matrix, 0)\n",
        "\n",
        "    # Compute density score for each image: here we take the average similarity\n",
        "    density_scores = similarity_matrix.mean(axis=1)\n",
        "\n",
        "    # Select the indices of the top-k samples with the highest density scores.\n",
        "    selected_indices = np.argsort(density_scores)[-k:]\n",
        "\n",
        "    return selected_indices, density_scores\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Test Code for Density-Based Sampling\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Here we assume that 'features' is already defined and has shape [24453, 512].\n",
        "# For testing purposes, if 'features' is not defined, we'll simulate a smaller set.\n",
        "try:\n",
        "    features.shape\n",
        "except NameError:\n",
        "    # For demonstration only: simulate a smaller features array (e.g., 50 images).\n",
        "    features = np.random.rand(50, 512)\n",
        "\n",
        "# Choose the number of samples you want to select.\n",
        "k = 5\n",
        "\n",
        "# Call the density-based sampling function.\n",
        "selected_indices, density_scores = select_density_samples_from_features(features, k)\n",
        "\n",
        "print(\"Selected indices (top {} samples):\".format(k), selected_indices)\n",
        "\n",
        "# Extract the density scores for only the selected samples.\n",
        "selected_density_scores = density_scores[selected_indices]\n",
        "\n",
        "print(\"Density scores for selected samples:\", selected_density_scores)\n",
        "\n",
        "# Plot the density scores for the selected samples only.\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(len(selected_density_scores)), selected_density_scores)\n",
        "plt.xlabel(\"Selected Sample (order)\")\n",
        "plt.ylabel(\"Density Score\")\n",
        "plt.title(\"Density Scores for Selected Samples\")\n",
        "# Optionally, label the x-axis ticks with the actual indices from the dataset.\n",
        "plt.xticks(range(len(selected_density_scores)), selected_indices)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "!pip install torchinfo\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_active_dataset():\n",
        "\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    active_train_dir = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    os.makedirs(active_train_dir, exist_ok=True)\n",
        "\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            os.makedirs(os.path.join(active_train_dir, cls), exist_ok=True)\n",
        "    print(f\"Complete creating train_dir {active_train_dir}\")\n",
        "\n",
        "    orig_val_dir = os.path.join(ORIGINAL_DATASET_DIR, VAL_SUBDIR)\n",
        "    active_val_dir = os.path.join(ACTIVE_DATASET_DIR, VAL_SUBDIR)\n",
        "    if os.path.exists(orig_val_dir):\n",
        "        if os.path.exists(active_val_dir):\n",
        "            shutil.rmtree(active_val_dir)\n",
        "        shutil.copytree(orig_val_dir, active_val_dir)\n",
        "\n",
        "    orig_test_dir = os.path.join(ORIGINAL_DATASET_DIR, TEST_SUBDIR)\n",
        "    active_test_dir = os.path.join(ACTIVE_DATASET_DIR, TEST_SUBDIR)\n",
        "    if os.path.exists(orig_test_dir):\n",
        "        if os.path.exists(active_test_dir):\n",
        "            shutil.rmtree(active_test_dir)\n",
        "        shutil.copytree(orig_test_dir, active_test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_samples():\n",
        "\n",
        "    samples = []\n",
        "    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n",
        "    for cls in os.listdir(orig_train_dir):\n",
        "        cls_path = os.path.join(orig_train_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            image_files = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n",
        "            for f in image_files:\n",
        "                samples.append((cls, f))\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "def copy_samples(sample_list):\n",
        "\n",
        "    for cls, file_name in sample_list:\n",
        "        src_path = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        dst_path = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copy(src_path, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "def stratified_sample(all_samples, n_initial):\n",
        "\n",
        "    samples_by_class = {}\n",
        "    for cls, filename in all_samples:\n",
        "        samples_by_class.setdefault(cls, []).append((cls, filename))\n",
        "\n",
        "    stratified = []\n",
        "    for cls, samples in samples_by_class.items():\n",
        "        stratified.append(random.choice(samples))\n",
        "    remaining_count = n_initial - len(stratified)\n",
        "    if remaining_count > 0:\n",
        "        remaining_samples = list(set(all_samples) - set(stratified))\n",
        "        additional_samples = random.sample(remaining_samples, remaining_count)\n",
        "        stratified.extend(additional_samples)\n",
        "\n",
        "    return stratified\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "ORIGINAL_DATASET_DIR = '/content/data_yolo'\n",
        "ACTIVE_DATASET_DIR = '/content/data_active'\n",
        "\n",
        "TRAIN_SUBDIR = 'train'\n",
        "VAL_SUBDIR = 'val'\n",
        "TEST_SUBDIR = 'test'\n",
        "Num_Train = 20103\n",
        "N_INITIAL = int(0.3 * Num_Train)\n",
        "N_PER_PHASE = int(0.1 * Num_Train)\n",
        "NUM_PHASES = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Density-based selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Initialize dataset\n",
        "setup_active_dataset()\n",
        "all_samples = get_all_samples()\n",
        "\n",
        "# Prepare CSV logging\n",
        "log_path = \"accuracy_log_density.csv\"\n",
        "with open(log_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Phase\", \"Num_Images\", \"Top-1 Accuracy\", \"Top-5 Accuracy\"])\n",
        "\n",
        "# Initial sampling and train\n",
        "current_sample_list = stratified_sample(all_samples, N_INITIAL)\n",
        "copy_samples(current_sample_list)\n",
        "\n",
        "print(\"Initial Epoch\")\n",
        "model = YOLO('yolo11n-cls.pt')\n",
        "results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=\"active-phase0\", project=\"runs/classify\")\n",
        "\n",
        "# Log initial accuracy\n",
        "acc1 = results.top1\n",
        "acc5 = results.top5\n",
        "\n",
        "with open(log_path, \"a\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([0, len(current_sample_list), acc1, acc5])\n",
        "\n",
        "# Compute remaining samples\n",
        "remaining_samples = list(set(all_samples) - set(current_sample_list))\n",
        "\n",
        "# Active Learning (Density-Based)\n",
        "for phase in range(1, NUM_PHASES):\n",
        "    n_to_add = min(N_PER_PHASE, len(remaining_samples))\n",
        "    if n_to_add <= 0:\n",
        "        print(\"No more samples\")\n",
        "        break\n",
        "\n",
        "    # Extract subset features\n",
        "    remaining_indices = [all_samples.index(s) for s in remaining_samples]\n",
        "    subset_features = features.numpy()[remaining_indices]\n",
        "\n",
        "    # Density-based to choose top-k\n",
        "    selected_indices, _ = select_density_samples_from_features(subset_features, k=n_to_add)\n",
        "    new_samples = [remaining_samples[i] for i in selected_indices]\n",
        "\n",
        "    # Update sample list, copy images\n",
        "    copy_samples(new_samples)\n",
        "    current_sample_list.extend(new_samples)\n",
        "    remaining_samples = list(set(remaining_samples) - set(new_samples))\n",
        "\n",
        "    print(f\"Phase {phase}: add {n_to_add} samples, total sample number: {len(current_sample_list)}。\")\n",
        "\n",
        "    # Retrain\n",
        "    model = YOLO(f\"runs/classify/active-phase{phase-1}/weights/last.pt\")\n",
        "    results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=f\"active-phase{phase}\", project=\"runs/classify\")\n",
        "\n",
        "    # Log accuracy\n",
        "    acc1 = results.top1\n",
        "    acc5 = results.top5\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([phase, len(current_sample_list), acc1, acc5])\n",
        "\n",
        "    print(f\"Phase {phase} training completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4280832,
          "sourceId": 7368427,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
