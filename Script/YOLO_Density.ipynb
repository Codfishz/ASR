{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APC17Tt6f5Bh","executionInfo":{"status":"ok","timestamp":1745191172750,"user_tz":240,"elapsed":18394,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"d220381a-bd80-467b-e608-b4c80f947ade"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"n5odPXUk4iso"},"source":["# Dataloader"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"34iJrhoU2IuR","executionInfo":{"status":"ok","timestamp":1745191182327,"user_tz":240,"elapsed":9578,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["import os\n","from PIL import Image, UnidentifiedImageError\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import LabelEncoder\n","from torchvision import transforms\n","from torchvision.models import resnet18, ResNet18_Weights\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4dkyvngVk6T","executionInfo":{"status":"ok","timestamp":1745191185011,"user_tz":240,"elapsed":2680,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"d1b06e3f-afc7-4438-f92b-77068683874e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ASR'...\n","warning: redirecting to https://github.com/Codfishz/ASR.git/\n","remote: Enumerating objects: 102, done.\u001b[K\n","remote: Counting objects: 100% (102/102), done.\u001b[K\n","remote: Compressing objects: 100% (78/78), done.\u001b[K\n","remote: Total 102 (delta 52), reused 69 (delta 23), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (102/102), 12.27 MiB | 13.41 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n"]}],"source":["# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n","os.environ['GITHUB_TOKEN'] = \"\"\n","\n","GITHUB_USERNAME = \"Codfishz\"\n","REPO_NAME       = \"ASR\"\n","TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n","repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n","!git clone {repo_url}"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyCemzBtVk6T","executionInfo":{"status":"ok","timestamp":1745191185613,"user_tz":240,"elapsed":600,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"fab6f7b7-312f-46a0-d4bc-e124d32cd098"},"outputs":[{"output_type":"stream","name":"stdout","text":["warning: redirecting to https://github.com/Codfishz/ASR.git/\n","Already up to date.\n"]}],"source":["!cd {REPO_NAME} && git pull"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OvFAb4GKVk6U","executionInfo":{"status":"ok","timestamp":1745191185645,"user_tz":240,"elapsed":12,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["os.chdir('ASR/Script')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8yE4LJAVk6U","executionInfo":{"status":"ok","timestamp":1745191296740,"user_tz":240,"elapsed":111094,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"7e32d4e2-d466-4b48-b8f5-ddd899086d90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kaggle==1.7.4.2\n","  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n","Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n","\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/173.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.7.4.2\n","    Uninstalling kaggle-1.7.4.2:\n","      Successfully uninstalled kaggle-1.7.4.2\n","Successfully installed kaggle-1.7.4.2\n","Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/crop-pest-and-disease-detection\n","License(s): CC0-1.0\n","âœ… Total images found in dataset: 25220\n","Images before filter: 25220\n"]},{"output_type":"stream","name":"stderr","text":["Saving train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20176/20176 [00:31<00:00, 639.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","train split summary:\n","  Total images: 20093\n","\n"]},{"output_type":"stream","name":"stderr","text":["Saving val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2522/2522 [00:04<00:00, 605.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","val split summary:\n","  Total images: 2514\n","\n"]},{"output_type":"stream","name":"stderr","text":["Saving test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2522/2522 [00:04<00:00, 596.57it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","test split summary:\n","  Total images: 2519\n","\n","  Images after filter : 25126\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["%run \"Datapreparation_YOLO.ipynb\""]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-a7Vh0gocUZ","outputId":"77c5ca31-ff03-42f6-de83-26674b564731","executionInfo":{"status":"ok","timestamp":1745191332197,"user_tz":240,"elapsed":35454,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing 'Cashew anthracnose': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1729/1729 [00:02<00:00, 659.79it/s]\n","Processing 'Cashew gumosis': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392/392 [00:00<00:00, 620.88it/s]\n","Processing 'Cashew healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1368/1368 [00:01<00:00, 701.56it/s]\n","Processing 'Cashew leaf miner': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1378/1378 [00:01<00:00, 701.96it/s]\n","Processing 'Cashew red rust': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1682/1682 [00:02<00:00, 619.63it/s]\n","Processing 'Cassava bacterial blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2614/2614 [00:03<00:00, 722.93it/s]\n","Processing 'Cassava brown spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1481/1481 [00:02<00:00, 718.19it/s]\n","Processing 'Cassava green mite': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1015/1015 [00:01<00:00, 713.19it/s]\n","Processing 'Cassava healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1193/1193 [00:01<00:00, 731.33it/s]\n","Processing 'Cassava mosaic': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1205/1205 [00:01<00:00, 744.87it/s]\n","Processing 'Maize fall armyworm': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 746.24it/s]\n","Processing 'Maize grasshoper': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 795.22it/s]\n","Processing 'Maize healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:00<00:00, 767.08it/s]\n","Processing 'Maize leaf beetle': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 948/948 [00:01<00:00, 869.72it/s]\n","Processing 'Maize leaf blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1006/1006 [00:01<00:00, 694.61it/s]\n","Processing 'Maize leaf spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1259/1259 [00:01<00:00, 697.17it/s]\n","Processing 'Maize streak virus': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 979/979 [00:01<00:00, 681.07it/s]\n","Processing 'Tomato healthy': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 470/470 [00:00<00:00, 744.73it/s]\n","Processing 'Tomato leaf blight': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1301/1301 [00:01<00:00, 730.20it/s]\n","Processing 'Tomato leaf curl': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 754.14it/s]\n","Processing 'Tomato septoria leaf spot': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2743/2743 [00:03<00:00, 742.67it/s]\n","Processing 'Tomato verticulium wilt': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 773/773 [00:01<00:00, 737.29it/s]\n"]}],"source":["# Categories\n","categories = sorted([d for d in os.listdir(original_base) if os.path.isdir(os.path.join(original_base, d))])\n","\n","# Set image path and labels\n","image_paths = []\n","image_labels = []\n","\n","base_path = \"/content/data\"\n","\n","for category in categories:\n","    category_dir = os.path.join(base_path, category)\n","    filenames = [f for f in os.listdir(category_dir) if f.endswith(\".jpg\")]\n","    for filename in tqdm(filenames, desc=f\"Processing '{category}'\"):\n","      image_path = os.path.join(category_dir, filename)\n","      try:\n","          img = Image.open(image_path)\n","          img = img.convert(\"RGB\")\n","          img.load()  # load image each time to remove the damaged image thoroughly\n","          image_paths.append(image_path)\n","          image_labels.append(category)\n","      except (UnidentifiedImageError, OSError):\n","          continue\n","\n","# Label encoder\n","label_encoder = LabelEncoder()\n","encoded_labels = label_encoder.fit_transform(image_labels)\n","\n","# Dataset\n","class CropDiseaseDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Augmentation\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # To fit the pretrain model (è¯´æ˜¯resnetçš„å®˜æ–¹æ¨èè¾“å…¥å¤§å°)\n","    # transforms.RandomRotation(20),\n","    # transforms.ColorJitter(brightness=0.1, contrast=0.1),\n","    # transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # To fit the pretrain model (æ²¡æœ‰å¾ˆæ‡‚å•Šï¼ŒåŠ ä¸Šä¹‹åå›¾ç‰‡ç¾¤é­”ä¹±èˆ)\n","])\n","\n","\n","# Make Dataset and DataLoader\n","dataset = CropDiseaseDataset(image_paths, encoded_labels, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n","\n","# Visualization\n","# plt.figure(figsize=(10, 10))\n","# for images, labels in dataloader:\n","#     for i in range(9):\n","#         img = images[i].permute(1, 2, 0).numpy()\n","#         plt.subplot(3, 3, i+1)\n","#         plt.imshow(img)\n","#         plt.axis('off')\n","#     break\n","# plt.show()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eATpJjs4nQ_Y","outputId":"c54dc7e9-5ea2-47a1-8a55-e4aa889617bf","executionInfo":{"status":"ok","timestamp":1745191332208,"user_tz":240,"elapsed":8,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["25126\n"]}],"source":["print(len(dataset))"]},{"cell_type":"markdown","metadata":{"id":"VG8dsAAI4bQ7"},"source":["# Feature extraction (ResNet18)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4TBZQvu10LH","outputId":"854cc42a-efb2-45d1-aeaa-a56a79d1b9c5","executionInfo":{"status":"ok","timestamp":1745191430916,"user_tz":240,"elapsed":98706,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 230MB/s]\n","Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [01:37<00:00,  1.01it/s]\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load pretrained ResNet\n","weights = ResNet18_Weights.DEFAULT\n","resnet18 = resnet18(weights=weights)\n","\n","# Remove the final fully connected layer\n","feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n","feature_extractor.to(device)\n","feature_extractor.eval()\n","\n","# Extract features\n","\n","def extract_features(dataloader, model, device):\n","    all_features = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n","            images = images.to(device)\n","            features = model(images)                  # shape: [B, 512, 1, 1]\n","            features = features.view(features.size(0), -1)  # flatten to [B, 512]\n","            all_features.append(features.cpu())\n","            all_labels.append(labels)\n","\n","    return torch.cat(all_features), torch.cat(all_labels)\n","\n","features, labels = extract_features(dataloader, feature_extractor, device)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3Lolcnj4vos","outputId":"5b1ee64e-4823-4371-8584-4866d30b728e","executionInfo":{"status":"ok","timestamp":1745191430932,"user_tz":240,"elapsed":13,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Features shape: torch.Size([25126, 512])\n","Labels shape: torch.Size([25126])\n"]}],"source":["# Results\n","print(\"Features shape:\", features.shape)  # [N, 512]\n","print(\"Labels shape:\", labels.shape)      # [N]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"l_C56__M4v36","executionInfo":{"status":"ok","timestamp":1745191430955,"user_tz":240,"elapsed":22,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["def select_density_samples_from_features(features, k=10):\n","    \"\"\"\n","    Select the top-k samples based on density computed from pre-extracted features.\n","\n","    Parameters:\n","        features: A NumPy array of shape (N, D), where N is the number of images\n","                  (e.g. 24453) and D is the feature dimension (e.g. 512).\n","        k: The number of samples to select.\n","\n","    Returns:\n","        selected_indices: The indices of the selected samples (top-k with highest density).\n","        density_scores: A NumPy array of density scores for all samples.\n","    \"\"\"\n","    # Normalize the feature vectors to unit length (to use cosine similarity)\n","    norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8  # avoid division by zero\n","    norm_features = features / norms\n","\n","    # Compute the pairwise cosine similarity matrix.\n","    # Note: Similarity between feature i and j is the dot product of normalized features.\n","    similarity_matrix = np.dot(norm_features, norm_features.T)\n","\n","    # Remove self-similarity by setting the diagonal elements to 0.\n","    np.fill_diagonal(similarity_matrix, 0)\n","\n","    # Compute density score for each image: here we take the average similarity\n","    density_scores = similarity_matrix.mean(axis=1)\n","\n","    # Select the indices of the top-k samples with the highest density scores.\n","    selected_indices = np.argsort(density_scores)[-k:]\n","\n","    return selected_indices, density_scores\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6t84cNrOVk6X","executionInfo":{"status":"ok","timestamp":1745191499269,"user_tz":240,"elapsed":68312,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"32f48b4a-1356-43de-c90e-e84ee3437ece"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","Setup complete âœ… (12 CPUs, 53.0 GB RAM, 43.7/235.7 GB disk)\n"]}],"source":["!pip install ultralytics\n","!pip install torchinfo\n","\n","import ultralytics\n","ultralytics.checks()\n"]},{"cell_type":"markdown","metadata":{"id":"IBtqqZhMVk6X"},"source":["# Functions"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"LSbDNPPaVk6X","executionInfo":{"status":"ok","timestamp":1745191499344,"user_tz":240,"elapsed":62,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["def setup_active_dataset():\n","\n","    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n","    active_train_dir = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR)\n","    os.makedirs(active_train_dir, exist_ok=True)\n","\n","    for cls in os.listdir(orig_train_dir):\n","        cls_path = os.path.join(orig_train_dir, cls)\n","        if os.path.isdir(cls_path):\n","            os.makedirs(os.path.join(active_train_dir, cls), exist_ok=True)\n","    print(f\"Complete creating train_dir {active_train_dir}\")\n","\n","    orig_val_dir = os.path.join(ORIGINAL_DATASET_DIR, VAL_SUBDIR)\n","    active_val_dir = os.path.join(ACTIVE_DATASET_DIR, VAL_SUBDIR)\n","    if os.path.exists(orig_val_dir):\n","        if os.path.exists(active_val_dir):\n","            shutil.rmtree(active_val_dir)\n","        shutil.copytree(orig_val_dir, active_val_dir)\n","\n","    orig_test_dir = os.path.join(ORIGINAL_DATASET_DIR, TEST_SUBDIR)\n","    active_test_dir = os.path.join(ACTIVE_DATASET_DIR, TEST_SUBDIR)\n","    if os.path.exists(orig_test_dir):\n","        if os.path.exists(active_test_dir):\n","            shutil.rmtree(active_test_dir)\n","        shutil.copytree(orig_test_dir, active_test_dir)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"LWsTUCDFVk6Y","executionInfo":{"status":"ok","timestamp":1745191499348,"user_tz":240,"elapsed":2,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["def get_all_samples():\n","\n","    samples = []\n","    orig_train_dir = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR)\n","    for cls in os.listdir(orig_train_dir):\n","        cls_path = os.path.join(orig_train_dir, cls)\n","        if os.path.isdir(cls_path):\n","            image_files = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n","            for f in image_files:\n","                samples.append((cls, f))\n","    return samples"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"xEMoCCC7Vk6Y","executionInfo":{"status":"ok","timestamp":1745191499351,"user_tz":240,"elapsed":1,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["import shutil\n","def copy_samples(sample_list):\n","\n","    for cls, file_name in sample_list:\n","        src_path = os.path.join(ORIGINAL_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n","        dst_path = os.path.join(ACTIVE_DATASET_DIR, TRAIN_SUBDIR, cls, file_name)\n","        if not os.path.exists(dst_path):\n","            shutil.copy(src_path, dst_path)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Z3JIgEElVk6Y","executionInfo":{"status":"ok","timestamp":1745191499354,"user_tz":240,"elapsed":1,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["import random\n","def stratified_sample(all_samples, n_initial):\n","\n","    samples_by_class = {}\n","    for cls, filename in all_samples:\n","        samples_by_class.setdefault(cls, []).append((cls, filename))\n","\n","    stratified = []\n","    for cls, samples in samples_by_class.items():\n","        stratified.append(random.choice(samples))\n","    remaining_count = n_initial - len(stratified)\n","    if remaining_count > 0:\n","        remaining_samples = list(set(all_samples) - set(stratified))\n","        additional_samples = random.sample(remaining_samples, remaining_count)\n","        stratified.extend(additional_samples)\n","\n","    return stratified\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5M6GPmEfVk6Y"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"T-9Q6koEVk6Y","executionInfo":{"status":"ok","timestamp":1745191499357,"user_tz":240,"elapsed":2,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"outputs":[],"source":["# Hyperparameters\n","ORIGINAL_DATASET_DIR = '/content/data_yolo'\n","ACTIVE_DATASET_DIR = '/content/data_active'\n","\n","TRAIN_SUBDIR = 'train'\n","VAL_SUBDIR = 'val'\n","TEST_SUBDIR = 'test'\n","Num_Train = 20093\n","N_INITIAL = int(0.3 * Num_Train)\n","N_PER_PHASE = int(0.1 * Num_Train)\n","NUM_PHASES = 5"]},{"cell_type":"markdown","metadata":{"id":"yuv8_j6DVk6Z"},"source":["# Density-based selection"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNGjvhsiVk6Z","executionInfo":{"status":"ok","timestamp":1745192016037,"user_tz":240,"elapsed":516679,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"2969e0be-55a5-4860-b07c-06557417ad29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Complete creating train_dir /content/data_active/train\n","Initial Epoch\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.52M/5.52M [00:00<00:00, 328MB/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase0, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase0\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 6027 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n","Overriding model.yaml nc=80 with nc=22\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 234/236 items from pretrained weights\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n","âš ï¸ Download failure, retrying 1/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1859.4Â±674.6 MB/s, size: 60.5 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 6027 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6027/6027 [00:01<00:00, 3431.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1344.5Â±871.7 MB/s, size: 66.7 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<00:00, 3074.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data_active/val.cache\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase0\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.78G      3.207         16        640:   5%|â–         | 18/377 [00:04<00:37,  9.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.78G      3.199         16        640:   9%|â–‰         | 34/377 [00:06<00:28, 12.05it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 91.5MB/s]\n","        1/1      1.78G      2.032         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:37<00:00, 10.06it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.743      0.979\n","\n","1 epochs completed in 0.013 hours.\n","Optimizer stripped from runs/classify/active-phase0/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase0/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase0/weights/best.pt...\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 6027 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:06<00:00, 11.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.744      0.979\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase0\u001b[0m\n","Phase 1: add 2009 samples, total sample number: 8036ã€‚\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase0/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase1\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 8036 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2037.2Â±810.4 MB/s, size: 60.5 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 8036 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8036/8036 [00:02<00:00, 3518.44it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 633.5Â±104.3 MB/s, size: 66.7 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase1\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.46G     0.9089          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 503/503 [00:50<00:00, 10.05it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.785      0.996\n","\n","1 epochs completed in 0.017 hours.\n","Optimizer stripped from runs/classify/active-phase1/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase1/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase1/weights/best.pt...\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 8036 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 11.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.785      0.996\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase1\u001b[0m\n","Phase 1 training completed\n","Phase 2: add 2009 samples, total sample number: 10045ã€‚\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase1/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase2\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 10045 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1757.6Â±821.2 MB/s, size: 55.0 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 10045 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10045/10045 [00:02<00:00, 3490.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/data_active/train/Maize leaf beetle/leaf beetle325_.jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 680.9Â±371.1 MB/s, size: 66.7 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase2\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.75G      0.711         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 628/628 [01:00<00:00, 10.41it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.814      0.997\n","\n","1 epochs completed in 0.019 hours.\n","Optimizer stripped from runs/classify/active-phase2/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase2/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase2/weights/best.pt...\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 10045 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:06<00:00, 11.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.814      0.997\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase2\u001b[0m\n","Phase 2 training completed\n","Phase 3: add 2009 samples, total sample number: 12054ã€‚\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase2/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase3\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 12054 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1532.9Â±615.0 MB/s, size: 50.6 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 12054 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12054/12054 [00:03<00:00, 3513.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 620.2Â±89.3 MB/s, size: 66.7 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase3\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.57G     0.6008          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 754/754 [01:11<00:00, 10.60it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.824      0.999\n","\n","1 epochs completed in 0.023 hours.\n","Optimizer stripped from runs/classify/active-phase3/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase3/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase3/weights/best.pt...\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 12054 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.823      0.999\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase3\u001b[0m\n","Phase 3 training completed\n","Phase 4: add 2009 samples, total sample number: 14063ã€‚\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/active-phase3/weights/last.pt, data=/content/data_active, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/classify, name=active-phase4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/active-phase4\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 14063 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 10                  -1  1    358422  ultralytics.nn.modules.head.Classify         [256, 22]                     \n","YOLO11n-cls summary: 86 layers, 1,559,286 parameters, 1,559,286 gradients, 3.3 GFLOPs\n","Transferred 236/236 items from pretrained weights\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1562.8Â±446.1 MB/s, size: 54.0 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_active/train... 14063 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14063/14063 [00:04<00:00, 3478.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_active/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 567.6Â±78.5 MB/s, size: 66.7 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_active/val... 2514 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2514/2514 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/classify/active-phase4\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/1      1.57G     0.5387         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 879/879 [01:22<00:00, 10.69it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.846      0.998\n","\n","1 epochs completed in 0.026 hours.\n","Optimizer stripped from runs/classify/active-phase4/weights/last.pt, 3.2MB\n","Optimizer stripped from runs/classify/active-phase4/weights/best.pt, 3.2MB\n","\n","Validating runs/classify/active-phase4/weights/best.pt...\n","Ultralytics 8.3.111 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n","YOLO11n-cls summary (fused): 47 layers, 1,554,206 parameters, 0 gradients, 3.2 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/data_active/train... found 14063 images in 22 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m /content/data_active/val... found 2514 images in 22 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m /content/data_active/test... found 2519 images in 22 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:06<00:00, 11.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.846      0.998\n","Speed: 0.5ms preprocess, 0.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/active-phase4\u001b[0m\n","Phase 4 training completed\n"]}],"source":["from ultralytics import YOLO\n","import random\n","import os\n","\n","# Initialize dataset\n","setup_active_dataset()\n","all_samples = get_all_samples()\n","\n","# Initial sampling and train\n","current_sample_list = stratified_sample(all_samples, N_INITIAL)\n","copy_samples(current_sample_list)\n","\n","print(\"Initial Epoch\")\n","model = YOLO('yolo11n-cls.pt')\n","results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=\"active-phase0\", project=\"runs/classify\")\n","\n","# Compute remaining samples\n","remaining_samples = list(set(all_samples) - set(current_sample_list))\n","\n","# Active Learning (Density-Based)\n","for phase in range(1, NUM_PHASES):\n","    n_to_add = min(N_PER_PHASE, len(remaining_samples))\n","    if n_to_add <= 0:\n","        print(\"No more samples\")\n","        break\n","\n","    # Extract subset features\n","    remaining_indices = [all_samples.index(s) for s in remaining_samples]\n","    subset_features = features.numpy()[remaining_indices]\n","\n","    # Density-based to choose top-k\n","    selected_indices, _ = select_density_samples_from_features(subset_features, k=n_to_add)\n","    new_samples = [remaining_samples[i] for i in selected_indices]\n","\n","    # Update sample list, copy images\n","    copy_samples(new_samples)\n","    current_sample_list.extend(new_samples)\n","    remaining_samples = list(set(remaining_samples) - set(new_samples))\n","\n","    print(f\"Phase {phase}: add {n_to_add} samples, total sample number: {len(current_sample_list)}ã€‚\")\n","\n","    # Retrain\n","    model = YOLO(f\"runs/classify/active-phase{phase-1}/weights/last.pt\")\n","    results = model.train(data=ACTIVE_DATASET_DIR, epochs=1, imgsz=640, name=f\"active-phase{phase}\", project=\"runs/classify\")\n","\n","    print(f\"Phase {phase} training completed\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"pDF6WZQtVk6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745192016706,"user_tz":240,"elapsed":666,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}},"outputId":"798b77b6-e667-4e45-929a-0ebfdc10a40e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copied: confusion_matrix.png â†’ /content/drive/MyDrive/Colab Notebooks/ASR/results/confusion_matrix_density.png\n","Copied: confusion_matrix_normalized.png â†’ /content/drive/MyDrive/Colab Notebooks/ASR/results/confusion_matrix_normalized_density.png\n"]}],"source":["import pandas as pd\n","import csv\n","import os\n","import shutil\n","\n","# Save running log\n","log_path = \"/content/drive/MyDrive/Colab Notebooks/ASR/results/accuracy_log_density.csv\"\n","log_columns = [\"Phase\", \"Epoch\", \"Time\", \"Train Loss\", \"Val Loss\",\n","               \"Top-1 Accuracy\", \"Top-5 Accuracy\", \"LR_pg0\", \"LR_pg1\", \"LR_pg2\"]\n","with open(log_path, \"w\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Phase\", \"Num_Images\"] + log_columns[1:])\n","\n","# Traverse phase dic\n","for phase in range(NUM_PHASES):\n","    result_csv_path = os.path.join(\"runs\", \"classify\", f\"active-phase{phase}\", \"results.csv\")\n","    if not os.path.exists(result_csv_path):\n","        print(f\"Warning: {result_csv_path} not found.\")\n","        continue\n","\n","    df = pd.read_csv(result_csv_path)\n","    last_row = df.iloc[-1]\n","\n","    # Count num of samples\n","    num_images = N_INITIAL + phase * N_PER_PHASE\n","\n","    with open(log_path, \"a\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\n","            phase,\n","            num_images,\n","            last_row[\"epoch\"],\n","            last_row[\"time\"],\n","            last_row[\"train/loss\"],\n","            last_row[\"val/loss\"],\n","            last_row[\"metrics/accuracy_top1\"],\n","            last_row[\"metrics/accuracy_top5\"],\n","            last_row[\"lr/pg0\"],\n","            last_row[\"lr/pg1\"],\n","            last_row[\"lr/pg2\"]\n","        ])\n","\n","# Save confusion matrix\n","# Last phase\n","last_phase = NUM_PHASES - 1\n","\n","# Path\n","base_path = f\"/content/ASR/Script/runs/classify/active-phase{last_phase}\"\n","confusion_files = [\"confusion_matrix.png\", \"confusion_matrix_normalized.png\"]\n","\n","# Sey Google Drive dictionary\n","drive_target_dir = \"/content/drive/MyDrive/Colab Notebooks/ASR/results\"\n","\n","# Copy\n","for filename in confusion_files:\n","    src = os.path.join(base_path, filename)\n","    dst = os.path.join(drive_target_dir, f\"{filename[:-4]}_density.png\")\n","    if os.path.exists(src):\n","        shutil.copy(src, dst)\n","        print(f\"Copied: {filename} â†’ {dst}\")\n","    else:\n","        print(f\"File not found: {src}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Vfpxt9dGfgDb","executionInfo":{"status":"ok","timestamp":1745192016710,"user_tz":240,"elapsed":2,"user":{"displayName":"Gaole Lin","userId":"13531080570893436529"}}},"execution_count":19,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"collapsed_sections":["IBtqqZhMVk6X"]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4280832,"sourceId":7368427,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}